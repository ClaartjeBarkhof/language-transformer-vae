{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1016 14:05:56.439916 4676832704 filelock.py:274] Lock 140728789747752 acquired on /Users/claartje/.cache/torch/transformers/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.69e5e35e0b798cab5e473f253752f8bf4d280ee37682281a23eed80f6e2d09c6.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c30bc21e61434bb148a4d467ae8dd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=760.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1016 14:05:56.904492 4676832704 filelock.py:318] Lock 140728789747752 released on /Users/claartje/.cache/torch/transformers/c9cc6e53904f7f3679a31ec4af244f4419e25ebc8e71ebf8c558a31cbcf07fc8.69e5e35e0b798cab5e473f253752f8bf4d280ee37682281a23eed80f6e2d09c6.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/claartje/miniconda3/envs/thesisenv-local/lib/python3.6/site-packages/transformers/configuration_xlnet.py:212: FutureWarning: This config doesn't use attention memories, a core feature of XLNet. Consider setting `men_len` to a non-zero value, for example `xlnet = XLNetLMHeadModel.from_pretrained('xlnet-base-cased'', mem_len=1024)`, for accurate training performance as well as an order of magnitude faster inference. Starting from version 3.5.0, the default parameter will be 1024, following the implementation in https://arxiv.org/abs/1906.08237\n",
      "  FutureWarning,\n",
      "I1016 14:05:57.332174 4676832704 filelock.py:274] Lock 140727482736256 acquired on /Users/claartje/.cache/torch/transformers/dad589d582573df0293448af5109cb6981ca77239ed314e15ca63b7b8a318ddd.8b10bd978b5d01c21303cc761fc9ecd464419b3bf921864a355ba807cfbfafa8.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d1ba92986f2488f8e69f296526b8235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=798011.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1016 14:05:58.366622 4676832704 filelock.py:318] Lock 140727482736256 released on /Users/claartje/.cache/torch/transformers/dad589d582573df0293448af5109cb6981ca77239ed314e15ca63b7b8a318ddd.8b10bd978b5d01c21303cc761fc9ecd464419b3bf921864a355ba807cfbfafa8.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/claartje/miniconda3/envs/thesisenv-local/lib/python3.6/site-packages/transformers/modeling_auto.py:785: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n",
      "I1016 14:05:58.984509 4676832704 filelock.py:274] Lock 140722326870560 acquired on /Users/claartje/.cache/torch/transformers/33d6135fea0154c088449506a4c5f9553cb59b6fd040138417a7033af64bb8f9.7eac4fe898a021204e63c88c00ea68c60443c57f94b4bc3c02adbde6465745ac.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "769ad6bb325a4ebbadeb7b7a454decae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=467042463.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1016 14:06:39.557795 4676832704 filelock.py:318] Lock 140722326870560 released on /Users/claartje/.cache/torch/transformers/33d6135fea0154c088449506a4c5f9553cb59b6fd040138417a7033af64bb8f9.7eac4fe898a021204e63c88c00ea68c60443c57f94b4bc3c02adbde6465745ac.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead\n",
    "\n",
    "tokenizer_xlnet = AutoTokenizer.from_pretrained(\"xlnet-base-cased\")\n",
    "model_xlnet = AutoModelWithLMHead.from_pretrained(\"xlnet-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters XLNet 116750336\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'XLNetTokenizer' object has no attribute 'encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-6b0f59b14cf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of parameters XLNet\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_n_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_xlnet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of tokens in vocabulary XLNet\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_xlnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'XLNetTokenizer' object has no attribute 'encoder'"
     ]
    }
   ],
   "source": [
    "print(\"Number of parameters XLNet\", get_n_params(model_xlnet))\n",
    "print(\"Number of tokens in vocabulary XLNet\", len(tokenizer_xlnet.encoder.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'training': False,\n",
       " '_parameters': OrderedDict(),\n",
       " '_buffers': OrderedDict(),\n",
       " '_backward_hooks': OrderedDict(),\n",
       " '_forward_hooks': OrderedDict(),\n",
       " '_forward_pre_hooks': OrderedDict(),\n",
       " '_state_dict_hooks': OrderedDict(),\n",
       " '_load_state_dict_pre_hooks': OrderedDict(),\n",
       " '_modules': OrderedDict([('transformer',\n",
       "               XLNetModel(\n",
       "                 (word_embedding): Embedding(32000, 768)\n",
       "                 (layer): ModuleList(\n",
       "                   (0): XLNetLayer(\n",
       "                     (rel_attn): XLNetRelativeAttention(\n",
       "                       (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                     (ff): XLNetFeedForward(\n",
       "                       (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (1): XLNetLayer(\n",
       "                     (rel_attn): XLNetRelativeAttention(\n",
       "                       (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                     (ff): XLNetFeedForward(\n",
       "                       (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (2): XLNetLayer(\n",
       "                     (rel_attn): XLNetRelativeAttention(\n",
       "                       (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                     (ff): XLNetFeedForward(\n",
       "                       (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (3): XLNetLayer(\n",
       "                     (rel_attn): XLNetRelativeAttention(\n",
       "                       (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                     (ff): XLNetFeedForward(\n",
       "                       (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (4): XLNetLayer(\n",
       "                     (rel_attn): XLNetRelativeAttention(\n",
       "                       (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                     (ff): XLNetFeedForward(\n",
       "                       (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (5): XLNetLayer(\n",
       "                     (rel_attn): XLNetRelativeAttention(\n",
       "                       (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                     (ff): XLNetFeedForward(\n",
       "                       (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (6): XLNetLayer(\n",
       "                     (rel_attn): XLNetRelativeAttention(\n",
       "                       (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                     (ff): XLNetFeedForward(\n",
       "                       (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (7): XLNetLayer(\n",
       "                     (rel_attn): XLNetRelativeAttention(\n",
       "                       (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                     (ff): XLNetFeedForward(\n",
       "                       (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (8): XLNetLayer(\n",
       "                     (rel_attn): XLNetRelativeAttention(\n",
       "                       (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                     (ff): XLNetFeedForward(\n",
       "                       (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (9): XLNetLayer(\n",
       "                     (rel_attn): XLNetRelativeAttention(\n",
       "                       (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                     (ff): XLNetFeedForward(\n",
       "                       (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (10): XLNetLayer(\n",
       "                     (rel_attn): XLNetRelativeAttention(\n",
       "                       (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                     (ff): XLNetFeedForward(\n",
       "                       (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                   (11): XLNetLayer(\n",
       "                     (rel_attn): XLNetRelativeAttention(\n",
       "                       (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                     (ff): XLNetFeedForward(\n",
       "                       (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                       (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                       (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                       (dropout): Dropout(p=0.1, inplace=False)\n",
       "                     )\n",
       "                     (dropout): Dropout(p=0.1, inplace=False)\n",
       "                   )\n",
       "                 )\n",
       "                 (dropout): Dropout(p=0.1, inplace=False)\n",
       "               )),\n",
       "              ('lm_loss',\n",
       "               Linear(in_features=768, out_features=32000, bias=True))]),\n",
       " 'config': XLNetConfig {\n",
       "   \"architectures\": [\n",
       "     \"XLNetLMHeadModel\"\n",
       "   ],\n",
       "   \"attn_type\": \"bi\",\n",
       "   \"bi_data\": false,\n",
       "   \"bos_token_id\": 1,\n",
       "   \"clamp_len\": -1,\n",
       "   \"d_head\": 64,\n",
       "   \"d_inner\": 3072,\n",
       "   \"d_model\": 768,\n",
       "   \"dropout\": 0.1,\n",
       "   \"end_n_top\": 5,\n",
       "   \"eos_token_id\": 2,\n",
       "   \"ff_activation\": \"gelu\",\n",
       "   \"initializer_range\": 0.02,\n",
       "   \"layer_norm_eps\": 1e-12,\n",
       "   \"mem_len\": null,\n",
       "   \"model_type\": \"xlnet\",\n",
       "   \"n_head\": 12,\n",
       "   \"n_layer\": 12,\n",
       "   \"pad_token_id\": 5,\n",
       "   \"reuse_len\": null,\n",
       "   \"same_length\": false,\n",
       "   \"start_n_top\": 5,\n",
       "   \"summary_activation\": \"tanh\",\n",
       "   \"summary_last_dropout\": 0.1,\n",
       "   \"summary_type\": \"last\",\n",
       "   \"summary_use_proj\": true,\n",
       "   \"task_specific_params\": {\n",
       "     \"text-generation\": {\n",
       "       \"do_sample\": true,\n",
       "       \"max_length\": 250\n",
       "     }\n",
       "   },\n",
       "   \"untie_r\": true,\n",
       "   \"vocab_size\": 32000\n",
       " },\n",
       " 'attn_type': 'bi',\n",
       " 'same_length': False}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(model_xlnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters BERT 108.340804\n",
      "Number of tokens in vocabulary BERT 28996\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer_bert = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "model_bert = AutoModelForMaskedLM.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "print(\"Number of parameters BERT\", get_n_params(model_bert)/1e6)\n",
    "print(\"Number of tokens in vocabulary BERT\", len(tokenizer_bert.vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at wietsedv/bert-base-dutch-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters Dutch BERTje 109112880\n",
      "Number of tokens in vocabulary Dutch BERTje 30000\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer_bert_dutch = AutoTokenizer.from_pretrained(\"wietsedv/bert-base-dutch-cased\")\n",
    "model_bert_dutch = AutoModelForMaskedLM.from_pretrained(\"wietsedv/bert-base-dutch-cased\")\n",
    "\n",
    "print(\"Number of parameters Dutch BERTje\", get_n_params(model_bert_dutch))\n",
    "print(\"Number of tokens in vocabulary Dutch BERTje\", len(tokenizer_bert_dutch.vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embedding are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at pdelobelle/robbert-v2-dutch-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at pdelobelle/robbert-v2-dutch-base and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters RobBERT 116752130\n",
      "Number of tokens in vocabulary RobBERT 39982\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "\n",
    "tokenizer_robbert = RobertaTokenizer.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\")\n",
    "model_robbert = RobertaForSequenceClassification.from_pretrained(\"pdelobelle/robbert-v2-dutch-base\")\n",
    "\n",
    "print(\"Number of parameters RobBERT\", get_n_params(model_robbert))\n",
    "print(\"Number of tokens in vocabulary RobBERT\", len(tokenizer_robbert.encoder.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters RoBERTa 124.645632\n",
      "Number of tokens in vocabulary RoBERTa 50265\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "tokenizer_roberta = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model_roberta = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "print(\"Number of parameters RoBERTa\", get_n_params(model_roberta)/1e6)\n",
    "print(\"Number of tokens in vocabulary RoBERTa\", len(tokenizer_roberta.encoder.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaConfig {\n",
       "  \"attention_probs_dropout_prob\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"max_position_embeddings\": 512,\n",
       "  \"model_type\": \"roberta\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"type_vocab_size\": 2,\n",
       "  \"vocab_size\": 30522\n",
       "}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import RobertaConfig, RobertaModel\n",
    "# Initializing a RoBERTa configuration\n",
    "configuration = RobertaConfig()\n",
    "# Initializing a model from the configuration\n",
    "model = RobertaModel(configuration)\n",
    "# Accessing the model configuration\n",
    "configuration = model.config\n",
    "configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1016 13:16:36.839262 4676832704 filelock.py:274] Lock 140728025270256 acquired on /Users/claartje/.cache/torch/transformers/ee421a5bf82f3951a3dc15afe28cfc38fa559efa382304516ffa6ed293ab83e9.b479b28114c95b1bad59f014bb119d3a50b3c5e5c87d6b0e0090a60c36629a83.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c4f4eaff5234a1b9160e5c7f10fd9c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=875.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1016 13:16:37.353291 4676832704 filelock.py:318] Lock 140728025270256 released on /Users/claartje/.cache/torch/transformers/ee421a5bf82f3951a3dc15afe28cfc38fa559efa382304516ffa6ed293ab83e9.b479b28114c95b1bad59f014bb119d3a50b3c5e5c87d6b0e0090a60c36629a83.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1016 13:16:37.760485 4676832704 filelock.py:274] Lock 140728024977248 acquired on /Users/claartje/.cache/torch/transformers/91ff5e1b255f639e7da51ea8c6a52a43f727a92c6b663da5e6191631dae60c53.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c95917c94404bd5b6129ebbe86a0534",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=5069051.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1016 13:16:39.275153 4676832704 filelock.py:318] Lock 140728024977248 released on /Users/claartje/.cache/torch/transformers/91ff5e1b255f639e7da51ea8c6a52a43f727a92c6b663da5e6191631dae60c53.309f0c29486cffc28e1e40a2ab0ac8f500c203fe080b95f820aa9cb58e5b84ed.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1016 13:16:41.053649 4676832704 filelock.py:274] Lock 140728025270032 acquired on /Users/claartje/.cache/torch/transformers/609796900634a6be6fe33dcf4a637ab799fafe01b65b4d5682690d58995ada39.c7e58d3598471b931c8c2d436201b40863e68677eb231675c2264d6a82b2e7bb.lock\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94284550324546e0b3bc2b57bece5dbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=2239696720.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I1016 13:18:29.650166 4676832704 filelock.py:318] Lock 140728025270032 released on /Users/claartje/.cache/torch/transformers/609796900634a6be6fe33dcf4a637ab799fafe01b65b4d5682690d58995ada39.c7e58d3598471b931c8c2d436201b40863e68677eb231675c2264d6a82b2e7bb.lock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large-finetuned-conll02-dutch were not used when initializing XLMRobertaForMaskedLM: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForMaskedLM were not initialized from the model checkpoint at xlm-roberta-large-finetuned-conll02-dutch and are newly initialized: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters Dutch XLM-RoBERTa 560.142482\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'XLMRobertaTokenizer' object has no attribute 'encoder'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d30bf57eac7c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of parameters Dutch XLM-RoBERTa\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_n_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_xlm_roberta_dutch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of tokens in vocabulary Dutch XLM-RoBERTa\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_xlm_roberta_dutch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'XLMRobertaTokenizer' object has no attribute 'encoder'"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer_xlm_roberta_dutch = AutoTokenizer.from_pretrained(\"xlm-roberta-large-finetuned-conll02-dutch\")\n",
    "model_xlm_roberta_dutch = AutoModelForMaskedLM.from_pretrained(\"xlm-roberta-large-finetuned-conll02-dutch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters Dutch XLM-RoBERTa 560.142482\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of parameters Dutch XLM-RoBERTa\", get_n_params(model_xlm_roberta_dutch)/1e6)\n",
    "# print(\"Number of tokens in vocabulary Dutch XLM-RoBERTa\", len(tokenizer_xlm_roberta_dutch.encoder.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TransfoXLConfig, TransfoXLModel\n",
    "\n",
    "# Initializing a Transformer XL configuration\n",
    "configuration = TransfoXLConfig()\n",
    "# Initializing a model from the configuration\n",
    "model = TransfoXLModel(configuration)\n",
    "# Accessing the model configuration\n",
    "configuration = model.config\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n",
      "torch.FloatTensor\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "283.885936"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_n_params(model) / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N params: 283.885936\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TransfoXLModel(\n",
       "  (word_emb): AdaptiveEmbedding(\n",
       "    (emb_layers): ModuleList(\n",
       "      (0): Embedding(20000, 1024)\n",
       "      (1): Embedding(20000, 256)\n",
       "      (2): Embedding(160000, 64)\n",
       "      (3): Embedding(67735, 16)\n",
       "    )\n",
       "    (emb_projs): ParameterList(\n",
       "        (0): Parameter containing: [torch.FloatTensor of size 1024x1024]\n",
       "        (1): Parameter containing: [torch.FloatTensor of size 1024x256]\n",
       "        (2): Parameter containing: [torch.FloatTensor of size 1024x64]\n",
       "        (3): Parameter containing: [torch.FloatTensor of size 1024x16]\n",
       "    )\n",
       "  )\n",
       "  (drop): Dropout(p=0.1, inplace=False)\n",
       "  (layers): ModuleList(\n",
       "    (0): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (dropatt): Dropout(p=0.0, inplace=False)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (1): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (dropatt): Dropout(p=0.0, inplace=False)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (2): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (dropatt): Dropout(p=0.0, inplace=False)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (3): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (dropatt): Dropout(p=0.0, inplace=False)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (4): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (dropatt): Dropout(p=0.0, inplace=False)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (5): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (dropatt): Dropout(p=0.0, inplace=False)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (6): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (dropatt): Dropout(p=0.0, inplace=False)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (7): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (dropatt): Dropout(p=0.0, inplace=False)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (8): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (dropatt): Dropout(p=0.0, inplace=False)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (9): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (dropatt): Dropout(p=0.0, inplace=False)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (10): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (dropatt): Dropout(p=0.0, inplace=False)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (11): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (dropatt): Dropout(p=0.0, inplace=False)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (12): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (dropatt): Dropout(p=0.0, inplace=False)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (13): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (dropatt): Dropout(p=0.0, inplace=False)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (14): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (dropatt): Dropout(p=0.0, inplace=False)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (15): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (dropatt): Dropout(p=0.0, inplace=False)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (16): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (dropatt): Dropout(p=0.0, inplace=False)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (17): RelPartialLearnableDecoderLayer(\n",
       "      (dec_attn): RelPartialLearnableMultiHeadAttn(\n",
       "        (qkv_net): Linear(in_features=1024, out_features=3072, bias=False)\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "        (dropatt): Dropout(p=0.0, inplace=False)\n",
       "        (o_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (r_net): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "      )\n",
       "      (pos_ff): PositionwiseFF(\n",
       "        (CoreNet): Sequential(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Dropout(p=0.1, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pos_emb): PositionalEmbedding()\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"N params:\", get_n_params(model) /  1e6)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "232.75008\n"
     ]
    }
   ],
   "source": [
    "print(232750080/1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some Sony PlayStation gamers are being advised to stay away from the network because of a problem with the PlayStation 3 network.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/roberta2roberta_L-24_bbc\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/roberta2roberta_L-24_bbc\")\n",
    "\n",
    "article = \"\"\"The problem is affecting people using the older\n",
    "versions of the PlayStation 3, called the \"Fat\"\n",
    "model.The problem isn't affecting the newer PS3\n",
    "Slim systems that have been on sale since\n",
    "September last year.Sony have also said they are\n",
    "aiming to have the problem fixed shortly but is\n",
    "advising some users to avoid using their console\n",
    "for the time being.\"We hope to resolve this\n",
    "problem within the next 24 hours,\" a statement\n",
    "reads. \"In the meantime, if you have a model other\n",
    "than the new slim PS3, we advise that you do not\n",
    "use your PS3 system, as doing so may result in\n",
    "errors in some functionality, such as recording\n",
    "obtained trophies, and not being able to restore\n",
    "certain data.\"We believe we have identified that\n",
    "this problem is being caused by a bug in the clock\n",
    "functionality incorporated in the system.\"The\n",
    "PlayStation Network is used by millions of people\n",
    "around the world.It allows users to play their\n",
    "friends at games like Fifa over the internet and\n",
    "also do things like download software or visit\n",
    "online stores.\"\"\"\n",
    "\n",
    "input_ids = tokenizer(article, return_tensors=\"pt\").input_ids\n",
    "output_ids = model.generate(input_ids)[0]\n",
    "\n",
    "print(tokenizer.decode(output_ids, skip_special_tokens=True))\n",
    "# should output\n",
    "# Some Sony PlayStation gamers are being advised to stay away from the network because of a problem with the PlayStation 3 network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "455.263414"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_n_params(model) / 1e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef3961f17a194d2ebe4900313da2daa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=3435.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adc2f9b121b04cb9b34c3fb1334fa652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=845717.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e34099920c51402a8e2fa9c05fd6ece1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=1821418078.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fox is developing a two-hour remake of the 1975 cult classic. The special will be directed, executive-produced and choreographed by Kenneth Ortega. The special is timed to celebrate the 40th anniversary of the film, which has grossed more than $112 million.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/roberta2roberta_L-24_cnn_daily_mail\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/roberta2roberta_L-24_cnn_daily_mail\")\n",
    "\n",
    "article = \"\"\"    (The Hollywood Reporter)\"The Rocky Horror Picture\n",
    "Show\" is the latest musical getting the small-\n",
    "screen treatment. Fox is developing a two-hour\n",
    "remake of the 1975 cult classic to be directed,\n",
    "executive-produced and choreographed by Kenneth\n",
    "Ortega (\"High School Musical\"). The project,\n",
    "tentatively titled \"The Rocky Horror Picture Show\n",
    "Event,\" is casting-contingent. The special will be\n",
    "filmed in advance and not air live, but few\n",
    "details beyond that are known. In addition to\n",
    "Ortega, Gail Berman and Lou Adler, who produced\n",
    "the original film, are also attached as executive\n",
    "producers. The special will be produced by Fox 21\n",
    "Television Studios, and Berman's The Jackal Group.\n",
    "The special is timed to celebrate the 40th\n",
    "anniversary of the film, which has grossed more\n",
    "than $112 million and still plays in theaters\n",
    "across the country. TV premiere dates: The\n",
    "complete guide . This isn't the first stab at\n",
    "adapting \"The Rocky Horror Picture Show.\" In 2002,\n",
    "Fox unveiled plans for an adaptation timed to the\n",
    "30th anniversary that never came to fruition. The\n",
    "faces of pilot season 2015 . Fox's \"Glee\" covered\n",
    "several of the show's most popular songs for a\n",
    "Season 2 episode and even released a special \"The\n",
    "Rocky Horror Glee Show\" EP. There is no plan yet\n",
    "for when the adaptation will air. Fox also has a\n",
    "live musical production of \"Grease\", starring\n",
    "Julianne Hough and Vanessa Hudgens, scheduled to\n",
    "air on Jan. 31, 2016. Broadcast TV scorecard .\n",
    "Following in the footsteps of \"The Sound of Music\"\n",
    "and \"Peter Pan,\" NBC recently announced plans to\n",
    "air a live version of The Wiz later this year.\n",
    "Ortega's credits include \"Gilmore Girls,\" \"This Is\n",
    "It\" and \"Hocus Pocus.\" He is repped by Paradigm\n",
    "and Hanson, Jacobson. 2015 The Hollywood\n",
    "Reporter. All rights reserved.\"\"\"\n",
    "\n",
    "input_ids = tokenizer(article, return_tensors=\"pt\").input_ids\n",
    "output_ids = model.generate(input_ids)[0]\n",
    "print(tokenizer.decode(output_ids, skip_special_tokens=True))\n",
    "# should output\n",
    "# Fox is developing a two-hour remake of the 1975 cult classic. The special will be directed, executive-produced and choreographed by Kenneth Ortega. \n",
    "# The special is timed to celebrate the 40th anniversary of the film, which has grossed more than $112 million."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/claartje/.cache/torch/hub/huggingface_pytorch-transformers_master\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "standard_bert = torch.hub.load('huggingface/pytorch-transformers', 'model', 'bert-base-cased')\n",
    "# standard_gpt2 = torch.hub.load('huggingface/pytorch-transformers', 'model', 'gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108.310272\n"
     ]
    }
   ],
   "source": [
    "print(get_n_params(standard_bert)/1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/claartje/.cache/torch/hub/huggingface_pytorch-transformers_master\n"
     ]
    }
   ],
   "source": [
    "standard_roberta = torch.hub.load('huggingface/pytorch-transformers', 'model', 'roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124.645632\n"
     ]
    }
   ],
   "source": [
    "print(get_n_params(standard_roberta)/1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0ccd3541ef34c16b733bb20bda9998a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=411.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f9a2d99f2234eeb965f00cf7c79a165",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=213450.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7625f1533774fb1b59f2f5af1e9759a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=263273408.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "65.190912\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
    "\n",
    "model = AutoModel.from_pretrained(\"distilbert-base-cased\")\n",
    "\n",
    "print(get_n_params(model)/1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Due Due hurricane, Lobsterfest has been canceled, making Bob very happy about it.<::::> He decides to open Bob's Burgers for customers who were planning on going to Lobsterfest.com.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/roberta2roberta_L-24_wikisplit\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/roberta2roberta_L-24_wikisplit\")\n",
    "\n",
    "long_sentence = \"\"\"Due to the hurricane, Lobsterfest has been canceled, making Bob very happy about it and he decides to open Bob 's Burgers for customers who were planning on going to Lobsterfest.\"\"\"\n",
    "\n",
    "input_ids = tokenizer(long_sentence, return_tensors=\"pt\").input_ids\n",
    "output_ids = model.generate(input_ids)[0]\n",
    "print(tokenizer.decode(output_ids, skip_special_tokens=False))\n",
    "# should output\n",
    "# Due Due hurricane, Lobsterfest has been canceled, making Bob very happy about it. He decides to open B\n",
    "# ob's Burgers for customers who were planning on going to Lobsterfest.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
